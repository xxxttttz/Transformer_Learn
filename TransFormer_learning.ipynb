{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxCh94plbIHwT9DcFT0MCP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xxxttttz/Transformer_Learn/blob/main/TransFormer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rOE3xhnqiz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f38e63-9fb3-4ae0-efc1-972a401224cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3231, 0.3264, 0.3505, 0.0000],\n",
            "         [0.3196, 0.3230, 0.3573, 0.0000],\n",
            "         [0.3132, 0.3232, 0.3636, 0.0000],\n",
            "         [0.3186, 0.3247, 0.3567, 0.0000]],\n",
            "\n",
            "        [[0.4731, 0.5269, 0.0000, 0.0000],\n",
            "         [0.4686, 0.5314, 0.0000, 0.0000],\n",
            "         [0.4759, 0.5241, 0.0000, 0.0000],\n",
            "         [0.4757, 0.5243, 0.0000, 0.0000]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [1.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0655,  0.0211],\n",
              "         [-0.0649,  0.0243],\n",
              "         [-0.0638,  0.0277],\n",
              "         [-0.0647,  0.0242]],\n",
              "\n",
              "        [[-0.0447,  0.0773],\n",
              "         [-0.0125, -0.0775],\n",
              "         [-0.0446,  0.0761],\n",
              "         [-0.0447,  0.0761]],\n",
              "\n",
              "        [[-0.0485, -0.3144],\n",
              "         [-0.0485, -0.3144],\n",
              "         [-0.0485, -0.3144],\n",
              "         [ 0.0000,  0.0000]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class selfAttentionV4(nn.Module):\n",
        "  def __init__(self, hidden_dim,dropout_rate=0.1) -> None:\n",
        "     super().__init__()\n",
        "     self.hidden_dim = hidden_dim\n",
        "\n",
        "     self.Q = nn.Linear(hidden_dim,hidden_dim)\n",
        "     self.K = nn.Linear(hidden_dim,hidden_dim)\n",
        "     self.V = nn.Linear(hidden_dim,hidden_dim)\n",
        "\n",
        "     self.dropout_rate = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self,X,attention_mask=None):\n",
        "    #X shape(bitch seq hidden_dim)\n",
        "    Q = self.Q(X)\n",
        "    K = self.K(X)\n",
        "    V = self.V(X)\n",
        "    #shape bit seq seq\n",
        "    attention_weight = Q @ K.transpose(-2,-1) / math.sqrt(self.hidden_dim)\n",
        "\n",
        "    # mask shape (bit seq seq)\n",
        "    if attention_mask is not None:\n",
        "      attention_weight = attention_weight.masked_fill(\n",
        "          attention_mask == 0,\n",
        "          float(\"-inf\")\n",
        "      )\n",
        "    attention_weight = torch.softmax(attention_weight,-1)\n",
        "    print(attention_weight)\n",
        "    attention_weight = self.dropout_rate(attention_weight)\n",
        "    #bit seq hidden_idm\n",
        "    output = attention_weight @ V\n",
        "    return output\n",
        "\n",
        "#bit seq hidden_dim\n",
        "X = torch.rand(3,4,2)\n",
        "# shape (3,4)\n",
        "mask = torch.tensor([\n",
        "    [1,1,1,0],\n",
        "    [1,1,0,0],\n",
        "    [1,0,0,0]\n",
        "])\n",
        "#shape 3 4 4\n",
        "mask = mask.unsqueeze(1).repeat(1,4,1)\n",
        "\n",
        "net = selfAttentionV4(2)\n",
        "net(X,mask)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,hidden_dim,head_num):\n",
        "     super().__init__()\n",
        "\n",
        "     self.hidden_dim = hidden_dim\n",
        "     self.head_num = head_num\n",
        "     self.head_dim = hidden_dim // head_num\n",
        "\n",
        "     self.query_proj = nn.Linear(hidden_dim,hidden_dim)\n",
        "     self.key_proj = nn.Linear(hidden_dim,hidden_dim)\n",
        "     self.value_proj = nn.Linear(hidden_dim,hidden_dim)\n",
        "\n",
        "     self.att_dropout = nn.Dropout(0.1)\n",
        "\n",
        "     self.output_proj = nn.Linear(hidden_dim,hidden_dim)\n",
        "\n",
        "  def forward(self,X,attention_mask = None):\n",
        "    bitch_size,seq_len,_ = X.shape\n",
        "\n",
        "    Q = self.query_proj(X)\n",
        "    K = self.key_proj(X)\n",
        "    V = self.value_proj(X)\n",
        "\n",
        "    #shape (bit seq hidden_dim)=> (bit head_num seq head_dim)\n",
        "    Q = Q.view(bitch_size,seq_len,self.head_num,self.head_dim).transpose(1,2)\n",
        "    K = K.view(bitch_size,seq_len,self.head_num,self.head_dim).permute(\n",
        "        0,2,1,3\n",
        "    )\n",
        "    V = V.view(bitch_size,seq_len,self.head_num,self.head_dim).transpose(1,2)\n",
        "\n",
        "    #shape (bit head_num seq seq)\n",
        "\n",
        "    attention_weight = Q @ K.transpose(-1,-2)/math.sqrt(self.head_dim)\n",
        "    if attention_mask is not None:\n",
        "      attention_weight = attention_weight.masked_fill(\n",
        "          attention_mask==0,\n",
        "          float(\"-1e20\")\n",
        "      )\n",
        "\n",
        "\n",
        "    #softmax\n",
        "    attention_weight = torch.softmax(attention_weight,-1)\n",
        "    print(attention_weight)\n",
        "\n",
        "    attention_weight = self.att_dropout(attention_weight)\n",
        "\n",
        "    #shape bit head_num seq head_dim\n",
        "    print(attention_weight.shape)\n",
        "    print(V.shape)\n",
        "    out_mid = attention_weight @ V\n",
        "\n",
        "    out_mid = out_mid.transpose(1,2).contiguous()\n",
        "    output = out_mid.view(bitch_size,seq_len,-1)\n",
        "    print(f\"outputsize is {output.shape}\")#[3, 2, 128]\n",
        "    output = self.output_proj(output)\n",
        "    print(f\"output is {output}\")\n",
        "    return output\n",
        "\n",
        "attention_mask = (torch.tensor(\n",
        "        [\n",
        "            [0, 1],\n",
        "            [0, 0],\n",
        "            [1, 0],\n",
        "        ]\n",
        "    )\n",
        "    .unsqueeze(1)\n",
        "    .unsqueeze(2)\n",
        "    .expand(3, 8, 2, 2)\n",
        ")\n",
        "print(f\"attention_mask shape is {attention_mask.shape}\")\n",
        "x = torch.rand(3, 2, 128)\n",
        "net = MultiHeadAttention(128, 8)\n",
        "net(x, attention_mask).shape\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qi8WbqferDH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5590df06-3144-4aea-bdb1-4be7306e8b48"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_mask shape is torch.Size([3, 8, 2, 2])\n",
            "tensor([[[[0.0000, 1.0000],\n",
            "          [0.0000, 1.0000]],\n",
            "\n",
            "         [[0.0000, 1.0000],\n",
            "          [0.0000, 1.0000]],\n",
            "\n",
            "         [[0.0000, 1.0000],\n",
            "          [0.0000, 1.0000]],\n",
            "\n",
            "         [[0.0000, 1.0000],\n",
            "          [0.0000, 1.0000]],\n",
            "\n",
            "         [[0.0000, 1.0000],\n",
            "          [0.0000, 1.0000]],\n",
            "\n",
            "         [[0.0000, 1.0000],\n",
            "          [0.0000, 1.0000]],\n",
            "\n",
            "         [[0.0000, 1.0000],\n",
            "          [0.0000, 1.0000]],\n",
            "\n",
            "         [[0.0000, 1.0000],\n",
            "          [0.0000, 1.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.5000, 0.5000],\n",
            "          [0.5000, 0.5000]],\n",
            "\n",
            "         [[0.5000, 0.5000],\n",
            "          [0.5000, 0.5000]],\n",
            "\n",
            "         [[0.5000, 0.5000],\n",
            "          [0.5000, 0.5000]],\n",
            "\n",
            "         [[0.5000, 0.5000],\n",
            "          [0.5000, 0.5000]],\n",
            "\n",
            "         [[0.5000, 0.5000],\n",
            "          [0.5000, 0.5000]],\n",
            "\n",
            "         [[0.5000, 0.5000],\n",
            "          [0.5000, 0.5000]],\n",
            "\n",
            "         [[0.5000, 0.5000],\n",
            "          [0.5000, 0.5000]],\n",
            "\n",
            "         [[0.5000, 0.5000],\n",
            "          [0.5000, 0.5000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000],\n",
            "          [1.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [1.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [1.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [1.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [1.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [1.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [1.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [1.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
            "torch.Size([3, 8, 2, 2])\n",
            "torch.Size([3, 8, 2, 16])\n",
            "outputsize is torch.Size([3, 2, 128])\n",
            "output is tensor([[[-1.3425e-01,  1.0296e-01,  4.7437e-02, -9.0934e-02,  1.7748e-02,\n",
            "          -4.2607e-02, -5.1989e-01,  8.9942e-02,  1.3460e-02,  1.7697e-01,\n",
            "           3.1190e-01,  1.7564e-02,  1.8409e-02,  2.4662e-01,  1.1749e-01,\n",
            "          -1.3816e-01, -7.3000e-02,  4.2869e-01,  1.6866e-01,  4.5747e-01,\n",
            "           1.2067e-01,  1.2508e-01,  2.3172e-01, -2.9536e-03,  4.9662e-02,\n",
            "           3.7787e-02, -3.1654e-01,  3.4965e-02, -3.2422e-02, -1.1354e-01,\n",
            "           6.3052e-02,  1.9303e-01, -2.2557e-01, -1.1991e-01,  8.0398e-03,\n",
            "          -2.1191e-01,  2.0032e-01,  1.0524e-01, -3.6138e-01,  8.5716e-02,\n",
            "          -6.3286e-02,  9.5110e-03,  2.3192e-02,  1.9114e-01,  7.8221e-03,\n",
            "           3.9191e-02,  7.7711e-02,  2.7378e-01, -2.8218e-01,  1.1761e-01,\n",
            "           1.9807e-01, -3.9121e-01, -3.5605e-01,  3.0140e-01,  3.7706e-01,\n",
            "           1.0102e-01, -4.8459e-02, -1.5361e-01, -5.8479e-02, -2.2174e-01,\n",
            "           3.2152e-01, -6.8907e-03, -2.1274e-01,  3.8285e-02, -1.9894e-01,\n",
            "          -3.6765e-01,  4.5838e-02,  1.3711e-01,  4.3199e-02,  3.6318e-02,\n",
            "          -2.0813e-01,  1.3325e-01, -1.3243e-01,  2.5328e-01,  1.0845e-01,\n",
            "          -1.0190e-01,  1.4824e-01,  4.0826e-01,  1.9669e-01,  2.9798e-01,\n",
            "           2.0368e-01,  5.4298e-02, -3.9892e-01,  8.8261e-02, -3.2801e-01,\n",
            "           9.3328e-02,  2.9447e-01,  2.7844e-01, -8.0174e-03,  3.1287e-01,\n",
            "           2.1426e-01, -6.4582e-02, -2.2812e-01,  2.2028e-01, -1.6665e-01,\n",
            "           1.5084e-01, -1.2860e-01, -5.2440e-01,  1.1478e-01, -3.1775e-01,\n",
            "           3.7084e-01,  4.9421e-01,  6.2818e-02, -7.1550e-03,  4.7925e-01,\n",
            "           2.0076e-01,  3.7703e-01, -1.3124e-01, -6.5290e-02, -6.9423e-02,\n",
            "           2.6247e-01,  1.8094e-02, -7.7554e-02,  1.3894e-01,  2.0305e-01,\n",
            "           2.7680e-01,  4.8855e-03, -2.0090e-01,  4.0207e-01, -3.6687e-01,\n",
            "          -3.7548e-02, -8.5758e-02, -2.7376e-01,  1.2114e-01,  1.3061e-01,\n",
            "          -2.6784e-01,  4.4126e-03,  1.6419e-01],\n",
            "         [-1.4615e-01, -8.5662e-02, -2.8251e-02,  1.5210e-01,  2.1298e-03,\n",
            "           1.0929e-01, -6.1993e-01,  1.0171e-02, -1.2918e-01,  1.2864e-01,\n",
            "           1.6419e-01,  1.0529e-01,  1.1424e-01,  1.7174e-01,  3.8575e-02,\n",
            "           7.6051e-03, -2.9873e-02,  4.7786e-01,  2.0454e-01,  5.3344e-01,\n",
            "           2.3223e-02,  2.9501e-01,  8.9675e-02,  2.2918e-01, -9.7349e-02,\n",
            "           1.9816e-02, -2.6819e-01,  8.0940e-02,  1.2778e-01, -3.0611e-02,\n",
            "           1.7911e-01, -2.0630e-02, -3.1799e-01, -1.9612e-01,  1.5082e-01,\n",
            "          -3.8910e-01,  8.2431e-02,  3.5479e-03, -3.3394e-01,  1.5882e-01,\n",
            "          -1.5250e-01, -6.0461e-02, -1.0664e-01,  3.7849e-01,  8.1114e-03,\n",
            "           5.5079e-02,  7.9328e-02,  2.3918e-01, -1.8073e-01, -1.0794e-01,\n",
            "           2.4852e-01,  4.9616e-02, -2.8504e-01,  2.1154e-01,  2.9213e-01,\n",
            "           8.3570e-03,  1.9935e-02, -9.1166e-02,  1.7653e-02, -2.4757e-01,\n",
            "           1.9572e-01,  1.4417e-01,  3.1553e-01, -8.7535e-02,  7.7207e-03,\n",
            "          -2.7506e-01, -1.0660e-01,  3.0817e-01,  8.6642e-02, -1.9399e-01,\n",
            "           6.3004e-02, -8.7147e-02, -1.2023e-01,  2.8760e-01, -7.8174e-02,\n",
            "          -4.0389e-01,  6.0385e-02,  2.8805e-01, -2.3456e-02,  2.5349e-01,\n",
            "           2.0743e-01, -5.2579e-02, -3.4834e-01,  1.2042e-01, -3.4130e-01,\n",
            "           3.8797e-02,  1.6951e-02,  4.2125e-01, -3.2527e-02,  1.6972e-01,\n",
            "           3.8058e-01,  9.3272e-02, -2.0633e-01,  3.4938e-02, -2.6830e-01,\n",
            "           1.5132e-01, -1.6524e-01, -5.2674e-01, -4.3087e-03, -2.7777e-01,\n",
            "           4.0456e-01,  5.4109e-01,  1.6481e-01,  1.1941e-01,  4.5648e-01,\n",
            "           6.0294e-02,  3.3056e-01, -2.1669e-01, -1.5017e-01, -7.4901e-02,\n",
            "           3.4672e-01,  3.2295e-02, -4.6122e-02,  2.7385e-01,  1.7093e-01,\n",
            "           3.2174e-01, -1.0481e-02,  3.0942e-02,  3.0806e-01, -3.3385e-01,\n",
            "           7.7042e-02, -5.0949e-02, -3.2553e-01,  1.3127e-02,  5.3063e-02,\n",
            "          -4.3351e-01,  1.6292e-01,  6.1760e-02]],\n",
            "\n",
            "        [[-2.8696e-02,  9.5230e-02, -7.5762e-02,  5.9711e-03, -1.5131e-01,\n",
            "           3.7391e-02, -4.4487e-01,  9.6743e-02,  1.8873e-01,  1.7813e-01,\n",
            "          -5.5024e-02,  3.5899e-02, -4.5207e-02,  1.2373e-01,  2.1723e-01,\n",
            "          -3.3728e-01, -1.4908e-01,  5.3317e-01,  3.6098e-01,  1.0249e+00,\n",
            "          -2.0655e-03,  6.0646e-01,  3.1829e-01,  1.8048e-01,  1.4417e-01,\n",
            "           4.8809e-02, -1.7180e-01, -1.0344e-01,  9.8593e-02, -5.1761e-02,\n",
            "           2.0331e-01,  1.2885e-01, -5.6786e-01, -3.4854e-02,  2.2070e-01,\n",
            "          -2.5521e-01,  1.2074e-01, -1.1530e-01, -4.3698e-01,  2.1128e-01,\n",
            "          -5.5558e-02, -6.1123e-02, -6.8387e-02,  3.0424e-01, -3.3351e-01,\n",
            "           3.5442e-03,  8.3950e-02,  4.8368e-01, -2.7027e-01,  5.0108e-02,\n",
            "           2.5465e-01, -2.2971e-01, -5.6086e-02,  6.1181e-03,  1.9102e-01,\n",
            "           6.7390e-02, -1.4827e-02, -2.0328e-01, -1.9639e-01, -4.5272e-01,\n",
            "           1.6943e-01,  1.9013e-01,  4.4001e-02,  9.0947e-02,  1.6748e-01,\n",
            "          -3.0784e-01, -2.0263e-01,  2.2052e-01,  9.2356e-03, -8.2708e-02,\n",
            "           3.0356e-01, -4.9532e-02, -7.5085e-03,  4.1046e-01,  3.8493e-02,\n",
            "          -4.5011e-01,  3.7202e-02,  3.9647e-01,  1.5656e-01,  1.9820e-01,\n",
            "           1.9404e-01, -6.8769e-03, -3.1232e-01, -5.2418e-03, -6.0714e-01,\n",
            "          -9.7500e-02,  7.2905e-02,  4.5269e-01, -8.7510e-02,  8.3397e-02,\n",
            "           3.5118e-01,  1.4674e-02, -2.3880e-01,  2.3994e-01, -1.5126e-01,\n",
            "           2.0700e-01,  1.2579e-01, -3.3975e-01,  6.4879e-02, -3.2094e-01,\n",
            "           4.6026e-01,  4.8287e-01,  3.3708e-01, -5.6303e-02,  5.5812e-01,\n",
            "           5.9800e-02,  2.4327e-01, -4.8179e-02, -2.7368e-01, -8.2792e-02,\n",
            "           1.4136e-01,  4.0677e-02, -6.5645e-02,  1.9482e-01,  2.2280e-01,\n",
            "           5.0565e-01,  5.1981e-02, -2.0490e-01,  5.4418e-01, -2.4595e-01,\n",
            "          -1.5896e-01, -2.1742e-01, -3.7432e-01,  4.9945e-02,  1.1655e-02,\n",
            "          -1.8991e-01,  4.7207e-02,  1.4690e-01],\n",
            "         [-2.8696e-02,  9.5230e-02, -7.5762e-02,  5.9711e-03, -1.5131e-01,\n",
            "           3.7391e-02, -4.4487e-01,  9.6743e-02,  1.8873e-01,  1.7813e-01,\n",
            "          -5.5024e-02,  3.5899e-02, -4.5207e-02,  1.2373e-01,  2.1723e-01,\n",
            "          -3.3728e-01, -1.4908e-01,  5.3317e-01,  3.6098e-01,  1.0249e+00,\n",
            "          -2.0655e-03,  6.0646e-01,  3.1829e-01,  1.8048e-01,  1.4417e-01,\n",
            "           4.8809e-02, -1.7180e-01, -1.0344e-01,  9.8593e-02, -5.1761e-02,\n",
            "           2.0331e-01,  1.2885e-01, -5.6786e-01, -3.4854e-02,  2.2070e-01,\n",
            "          -2.5521e-01,  1.2074e-01, -1.1530e-01, -4.3698e-01,  2.1128e-01,\n",
            "          -5.5558e-02, -6.1123e-02, -6.8387e-02,  3.0424e-01, -3.3351e-01,\n",
            "           3.5442e-03,  8.3950e-02,  4.8368e-01, -2.7027e-01,  5.0108e-02,\n",
            "           2.5465e-01, -2.2971e-01, -5.6086e-02,  6.1181e-03,  1.9102e-01,\n",
            "           6.7390e-02, -1.4827e-02, -2.0328e-01, -1.9639e-01, -4.5272e-01,\n",
            "           1.6943e-01,  1.9013e-01,  4.4001e-02,  9.0947e-02,  1.6748e-01,\n",
            "          -3.0784e-01, -2.0263e-01,  2.2052e-01,  9.2356e-03, -8.2708e-02,\n",
            "           3.0356e-01, -4.9532e-02, -7.5085e-03,  4.1046e-01,  3.8493e-02,\n",
            "          -4.5011e-01,  3.7202e-02,  3.9647e-01,  1.5656e-01,  1.9820e-01,\n",
            "           1.9404e-01, -6.8769e-03, -3.1232e-01, -5.2418e-03, -6.0714e-01,\n",
            "          -9.7500e-02,  7.2905e-02,  4.5269e-01, -8.7510e-02,  8.3397e-02,\n",
            "           3.5118e-01,  1.4674e-02, -2.3880e-01,  2.3994e-01, -1.5126e-01,\n",
            "           2.0700e-01,  1.2579e-01, -3.3975e-01,  6.4879e-02, -3.2094e-01,\n",
            "           4.6026e-01,  4.8287e-01,  3.3708e-01, -5.6303e-02,  5.5812e-01,\n",
            "           5.9800e-02,  2.4327e-01, -4.8179e-02, -2.7368e-01, -8.2792e-02,\n",
            "           1.4136e-01,  4.0677e-02, -6.5645e-02,  1.9482e-01,  2.2280e-01,\n",
            "           5.0565e-01,  5.1981e-02, -2.0490e-01,  5.4418e-01, -2.4595e-01,\n",
            "          -1.5896e-01, -2.1742e-01, -3.7432e-01,  4.9945e-02,  1.1655e-02,\n",
            "          -1.8991e-01,  4.7207e-02,  1.4690e-01]],\n",
            "\n",
            "        [[-2.0691e-01,  2.4244e-01, -9.2217e-03,  4.5251e-02, -5.1416e-03,\n",
            "           1.0359e-01, -4.1453e-01,  2.3707e-01, -1.0216e-01,  1.3017e-01,\n",
            "           2.1523e-01,  1.1904e-01,  2.0692e-01, -1.6074e-02, -3.1755e-02,\n",
            "          -1.5615e-02,  7.0082e-02,  2.9842e-01,  2.3539e-01,  8.2272e-01,\n",
            "           1.5302e-01,  1.3869e-01,  3.6261e-01, -1.4866e-01,  1.6237e-01,\n",
            "          -1.2154e-01, -4.9695e-01, -1.9776e-01,  8.4569e-02,  1.3823e-01,\n",
            "           2.8495e-01,  2.9156e-01, -4.2676e-01,  6.6015e-03,  3.0269e-01,\n",
            "          -7.6527e-02,  1.3540e-01, -1.0297e-01, -1.3268e-01,  1.7052e-01,\n",
            "          -2.0521e-01, -8.8353e-02,  1.5980e-01,  2.2202e-02, -4.8714e-01,\n",
            "          -2.2409e-02,  1.8486e-01,  1.1411e-01, -1.9407e-01, -3.0182e-02,\n",
            "           2.4087e-01, -7.8842e-02, -1.2205e-01,  5.0513e-03,  6.4934e-01,\n",
            "          -8.9115e-02, -3.5995e-02, -1.5603e-01,  9.6389e-02, -1.9362e-01,\n",
            "           2.8982e-01, -9.8762e-02, -1.2643e-02,  2.3007e-01, -1.4521e-02,\n",
            "          -3.7333e-01,  1.0387e-01,  3.6070e-01,  2.2941e-01, -1.0460e-02,\n",
            "           1.7706e-01,  1.4191e-02, -5.7237e-03,  2.8552e-01,  1.2277e-01,\n",
            "          -2.0608e-01, -3.4413e-02,  2.3902e-01,  1.8986e-01,  8.6398e-02,\n",
            "          -8.6607e-02,  1.9470e-01, -5.0352e-01,  8.6270e-02, -5.0395e-01,\n",
            "           1.3235e-01,  1.9929e-01,  2.4908e-01, -9.0214e-02,  3.0692e-01,\n",
            "           3.5291e-01,  1.1917e-01, -1.4538e-01,  1.6449e-01, -1.3633e-01,\n",
            "           2.8073e-01,  2.0894e-02, -3.1226e-01,  1.3949e-01, -2.8713e-01,\n",
            "           2.8652e-01,  2.4916e-01,  1.7091e-01, -3.8361e-02,  5.9249e-01,\n",
            "           1.3999e-01,  3.4455e-01,  1.0702e-02,  1.5856e-01, -1.7802e-01,\n",
            "           3.2085e-02,  9.1897e-02, -4.1240e-01,  1.7684e-01,  3.8133e-01,\n",
            "           4.2216e-01,  1.6855e-01, -1.5553e-01,  6.5827e-01, -1.7443e-01,\n",
            "          -1.5057e-02,  4.6993e-02, -1.6832e-01, -2.6576e-02,  1.2373e-01,\n",
            "          -1.7323e-01, -2.5866e-02,  1.2142e-01],\n",
            "         [-1.6409e-01,  2.3218e-01, -1.4549e-01,  1.0977e-01, -1.2939e-01,\n",
            "           1.1563e-01, -4.0956e-01,  2.1683e-01, -8.7635e-02,  1.4573e-01,\n",
            "           7.9353e-02,  2.4485e-02,  3.0790e-01, -3.4767e-02,  6.9587e-02,\n",
            "          -1.7798e-02,  1.0834e-01,  3.2012e-01,  2.8601e-01,  8.2554e-01,\n",
            "           1.4992e-01,  2.7225e-01,  4.0778e-01,  3.7519e-03,  1.1048e-01,\n",
            "          -4.0978e-02, -3.9657e-01, -1.6904e-01,  1.3576e-01, -6.2358e-04,\n",
            "           3.2867e-01,  3.4219e-01, -5.4343e-01, -8.3355e-03,  2.6422e-01,\n",
            "          -1.3459e-01,  7.1815e-02, -5.3079e-02, -1.8402e-01,  2.0944e-01,\n",
            "          -9.0484e-02,  5.5722e-02,  6.3513e-02, -3.9644e-02, -4.9890e-01,\n",
            "           3.9202e-03,  2.0207e-01,  2.9125e-01, -1.0203e-01,  2.7745e-02,\n",
            "           2.8887e-01,  2.8706e-02, -1.3652e-01, -1.4413e-02,  5.7686e-01,\n",
            "          -1.6549e-01, -2.1399e-02, -1.1478e-01,  1.1870e-01, -3.0712e-01,\n",
            "           3.2886e-01, -7.4301e-02,  1.6896e-01,  1.0751e-01,  1.0915e-01,\n",
            "          -3.2176e-01, -1.2787e-01,  3.5403e-01,  1.2007e-01, -8.0138e-02,\n",
            "           1.7957e-01,  3.7824e-02, -1.4442e-01,  3.5423e-01, -2.0064e-02,\n",
            "          -3.0359e-01, -1.0577e-01,  1.5571e-01, -4.3952e-02,  8.8662e-02,\n",
            "          -1.3148e-01,  9.8408e-02, -3.9055e-01,  1.1037e-01, -5.2504e-01,\n",
            "           1.2010e-01,  1.3544e-01,  3.6055e-01, -1.1387e-01,  2.5156e-01,\n",
            "           4.6766e-01,  1.4768e-01, -1.7115e-01,  2.0658e-01, -9.0465e-02,\n",
            "           2.9649e-01,  8.7491e-02, -3.4980e-01,  1.6971e-01, -2.4704e-01,\n",
            "           4.0810e-01,  3.7125e-01,  1.5002e-01, -1.7780e-02,  6.9503e-01,\n",
            "           1.0775e-01,  3.9463e-01, -6.2668e-02,  1.3317e-01, -1.2786e-01,\n",
            "           1.5896e-01, -4.9272e-02, -5.5545e-01,  2.7396e-01,  3.7023e-01,\n",
            "           5.0489e-01,  2.0285e-01, -1.2603e-01,  5.9488e-01, -2.5864e-01,\n",
            "          -1.7089e-02,  3.7865e-03, -2.5601e-01, -4.1705e-02,  2.3649e-02,\n",
            "          -3.2992e-01, -9.0271e-02,  2.2019e-01]]], grad_fn=<ViewBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlaJ6S7TytpT",
        "outputId": "b0010add-13fe-4639-a4e5-6b99d9dc6a61"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.15).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `newline'\n",
            "/bin/bash: -c: line 1: `git clone <https://github.com/xxxttttz/Transformer_Learn.git>'\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `newline'\n",
            "/bin/bash: -c: line 1: `git add <TransFormer_learning.ipynb>'\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    }
  ]
}